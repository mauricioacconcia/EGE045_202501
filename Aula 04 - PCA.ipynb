{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMgiLMc2JWZhElPWJsylpy6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Análise de Componentes Principais (PCA)\n","\n","## 1. Introdução\n","\n","A Análise de Componentes Principais (PCA) é uma técnica estatística e matemática utilizada para **redução de dimensionalidade** e **extração de padrões** em conjuntos de dados.  \n","O objetivo principal é transformar variáveis possivelmente correlacionadas em um novo conjunto de variáveis **não correlacionadas**, chamadas de **componentes principais**.\n","\n","Esses componentes principais são ordenados de forma que:\n","- O primeiro componente explica a maior parte da variabilidade dos dados.\n","- O segundo explica a maior parte do que resta, e assim por diante.\n","\n","### Motivação\n","- **Redução de dimensionalidade**: simplificar dados mantendo a maior parte da informação.  \n","- **Visualização**: projetar dados de alta dimensão em 2D ou 3D.  \n","- **Compressão**: representar dados com menos variáveis sem grande perda de informação.  \n","- **Eliminação de redundância**: variáveis altamente correlacionadas podem ser combinadas.\n","\n","### Exemplos práticos\n","- Reconhecimento de imagens e compressão (ex.: JPEG, faces).  \n","- Biologia e genética (analisar milhares de genes e encontrar padrões).  \n","- Recomendação de produtos (filtrar variáveis relevantes).  \n","- Sensoriamento em engenharia (combinar sinais de sensores correlacionados)."],"metadata":{"id":"wY4LZSisdJbC"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"OTd0es_idOow"}},{"cell_type":"markdown","source":["## 2. Pré-requisitos matemáticos\n","\n","Antes de compreender o PCA, é necessário revisar alguns conceitos fundamentais de Álgebra Linear.\n","\n","### 2.1 Vetores e Matrizes\n","- **Vetor**: coleção ordenada de números que pode representar um ponto ou direção em um espaço.\n","- **Matriz**: tabela de números organizada em linhas e colunas.\n","- **Operações básicas**: soma de vetores, multiplicação escalar, multiplicação de matrizes.\n","\n","Exemplo de vetor em R²:  \n","$$\n","\\vec{v} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}\n","$$\n","\n","Exemplo de matriz 2x2:  \n","$$\n","A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\n","$$\n","\n","---\n","\n","### 2.2 Produto Interno e Projeção\n","- O **produto interno** mede o quanto dois vetores \"apontam na mesma direção\".\n","- Para vetores $\\vec{u}$ e $\\vec{v}$:\n","\n","$$\n","\\vec{u} \\cdot \\vec{v} = \\| \\vec{u} \\| \\cdot \\| \\vec{v} \\| \\cdot \\cos(\\theta)\n","$$\n","\n","- A **projeção** de $\\vec{u}$ sobre $\\vec{v}$ é:\n","\n","$$\n","\\text{proj}_{\\vec{v}}(\\vec{u}) = \\frac{\\vec{u} \\cdot \\vec{v}}{\\| \\vec{v} \\|^2} \\cdot \\vec{v}\n","$$\n","\n","---\n","\n","### 2.3 Autovalores e Autovetores\n","- Um **autovetor** de uma matriz $A$ é um vetor $\\vec{x}$ que, ao ser multiplicado por $A$, apenas muda de escala.\n","- O valor dessa escala é o **autovalor** $\\lambda$.\n","\n","Formalmente:\n","\n","$$\n","A \\vec{x} = \\lambda \\vec{x}\n","$$\n","\n","- Isso significa que $\\vec{x}$ não muda de direção, apenas de magnitude.\n","\n","---\n","\n","### 2.4 Exemplo Numérico Simples\n","Considere a matriz:\n","\n","$$\n","A = \\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix}\n","$$\n","\n","- Os autovalores são $\\lambda_1 = 2$ e $\\lambda_2 = 3$.  \n","- Os autovetores correspondentes são $\\vec{x}_1 = [1, 0]^T$ e $\\vec{x}_2 = [0, 1]^T$.\n","\n","---\n","\n","### 2.5 Decomposição Espectral\n","- Matrizes quadradas podem ser decompostas em termos de autovalores e autovetores.\n","- Se $A$ é diagonalizável:\n","\n","$$\n","A = V \\Lambda V^{-1}\n","$$\n","\n","onde:\n","- $V$ é a matriz de autovetores.  \n","- $\\Lambda$ é a matriz diagonal com os autovalores.\n","\n","---\n","\n","### 2.6 Matrizes Simétricas\n","- Uma matriz simétrica é aquela em que $A = A^T$.  \n","- Propriedade importante: matrizes simétricas têm **autovalores reais** e **autovetores ortogonais**.\n","- Isso é fundamental no PCA, pois a matriz de covariância é simétrica."],"metadata":{"id":"RPBqt6TOdPcb"}},{"cell_type":"markdown","source":["## Exemplo passo a passo: autovalores e autovetores\n","\n","Vamos usar uma matriz simétrica simples, útil para a intuição do PCA.\n","\n","$$\n","A=\\begin{bmatrix}2 & 1\\\\[4pt] 1 & 2\\end{bmatrix}\n","$$\n","\n","### 1. Polinômio característico\n","Autovalores $\\lambda$ satisfazem $\\det(A-\\lambda I)=0$.\n","\n","$$\n","A-\\lambda I=\\begin{bmatrix}2-\\lambda & 1\\\\[4pt]1 & 2-\\lambda\\end{bmatrix}\n","\\quad\\Rightarrow\\quad\n","\\det(A-\\lambda I)=(2-\\lambda)^2-1\n","$$\n","\n","Expansão:\n","\n","$$\n","(2-\\lambda)^2-1\n","= \\lambda^2-4\\lambda+4-1\n","= \\lambda^2-4\\lambda+3\n","$$\n","\n","Logo, o polinômio característico é\n","$$\n","p(\\lambda)=\\lambda^2-4\\lambda+3\n","$$\n","\n","### 2. Autovalores\n","Resolva $p(\\lambda)=0$:\n","\n","$$\n","\\lambda^2-4\\lambda+3=0\n","\\quad\\Rightarrow\\quad\n","(\\lambda-1)(\\lambda-3)=0\n","$$\n","\n","Autovalores:\n","$$\n","\\lambda_1=1,\\qquad \\lambda_2=3\n","$$\n","\n","### 3. Autovetor para $\\lambda_2=3$\n","Resolva $(A-3I)\\vec{v}=\\vec{0}$.\n","\n","$$\n","A-3I=\\begin{bmatrix}-1 & 1\\\\[4pt] 1 & -1\\end{bmatrix}\n","$$\n","\n","Sistema:\n","$$\n","\\begin{cases}\n","- x + y = 0 \\\\\n","x - y = 0\n","\\end{cases}\n","\\quad\\Rightarrow\\quad y=x\n","$$\n","\n","Um autovetor é qualquer múltiplo de $\\begin{bmatrix}1\\\\[2pt]1\\end{bmatrix}$.\n","Vetor normalizado:\n","\n","$$\n","\\vec{v}_2=\\frac{1}{\\sqrt{2}}\\begin{bmatrix}1\\\\[2pt]1\\end{bmatrix}\n","$$\n","\n","Verificação:\n","\n","$$\n","A\\begin{bmatrix}1\\\\[2pt]1\\end{bmatrix}\n","=\\begin{bmatrix}2\\cdot1+1\\cdot1\\\\[2pt]1\\cdot1+2\\cdot1\\end{bmatrix}\n","=\\begin{bmatrix}3\\\\[2pt]3\\end{bmatrix}\n","=3\\begin{bmatrix}1\\\\[2pt]1\\end{bmatrix}\n","$$\n","\n","### 4. Autovetor para $\\lambda_1=1$\n","Resolva $(A-I)\\vec{v}=\\vec{0}$.\n","\n","$$\n","A-I=\\begin{bmatrix}1 & 1\\\\[4pt] 1 & 1\\end{bmatrix}\n","$$\n","\n","Sistema:\n","$$\n","\\begin{cases}\n","x + y = 0 \\\\\n","x + y = 0\n","\\end{cases}\n","\\quad\\Rightarrow\\quad y=-x\n","$$\n","\n","Um autovetor é qualquer múltiplo de $\\begin{bmatrix}1\\\\[2pt]-1\\end{bmatrix}$.\n","Vetor normalizado:\n","\n","$$\n","\\vec{v}_1=\\frac{1}{\\sqrt{2}}\\begin{bmatrix}1\\\\[2pt]-1\\end{bmatrix}\n","$$\n","\n","Verificação:\n","\n","$$\n","A\\begin{bmatrix}1\\\\[2pt]-1\\end{bmatrix}\n","=\\begin{bmatrix}2\\cdot1+1\\cdot(-1)\\\\[2pt]1\\cdot1+2\\cdot(-1)\\end{bmatrix}\n","=\\begin{bmatrix}1\\\\[2pt]-1\\end{bmatrix}\n","=1\\begin{bmatrix}1\\\\[2pt]-1\\end{bmatrix}\n","$$\n","\n","### 5. Ortonormalidade e diagonalização\n","Como $A$ é simétrica, autovetores correspondentes a autovalores distintos são ortogonais.\n","\n","$$\n","\\vec{v}_1^\\top \\vec{v}_2\n","=\\frac{1}{2}[1\\ \\ -1]\\begin{bmatrix}1\\\\[2pt]1\\end{bmatrix}\n","=\\frac{1}{2}(1-1)=0\n","$$\n","\n","Forme $V=[\\vec{v}_2\\ \\vec{v}_1]$ e $\\Lambda=\\operatorname{diag}(3,1)$.\n","\n","$$\n","V=\\frac{1}{\\sqrt{2}}\n","\\begin{bmatrix}\n","1 & 1\\\\[4pt]\n","1 & -1\n","\\end{bmatrix},\n","\\qquad\n","\\Lambda=\n","\\begin{bmatrix}\n","3 & 0\\\\[4pt]\n","0 & 1\n","\\end{bmatrix}\n","$$\n","\n","Diagonalização:\n","\n","$$\n","A = V\\Lambda V^\\top\n","$$\n","\n","### 6. Interpretação geométrica\n","O operador $A$ escala os vetores ao longo de duas direções principais:\n","$\\vec{v}_2$ (linha $y=x$) é ampliada por fator 3.\n","$\\vec{v}_1$ (linha $y=-x$) é mantida por fator 1.\n","\n","Para PCA isso é crucial, pois a matriz de covariância é simétrica e suas direções principais correspondem aos autovetores. Os maiores autovalores indicam eixos de maior variância.\n"],"metadata":{"id":"pWeYgqCGeQ_g"}},{"cell_type":"code","source":["import numpy as np\n","\n","def eigen_decomp(A):\n","    \"\"\"\n","    Decomposição espectral geral.\n","    Usa numpy.linalg.eig, serve para qualquer matriz quadrada.\n","    Retorna autovalores e autovetores ordenados do maior para o menor autovalor.\n","    \"\"\"\n","    vals, vecs = np.linalg.eig(A)\n","    idx = np.argsort(vals)[::-1]\n","    vals = np.real_if_close(vals[idx])\n","    vecs = np.real_if_close(vecs[:, idx])\n","\n","    # normaliza colunas para norma 1\n","    norms = np.linalg.norm(vecs, axis=0)\n","    norms[norms == 0] = 1.0\n","    vecs = vecs / norms\n","    return vals, vecs\n","\n","def eigen_decomp_symmetric(A):\n","    \"\"\"\n","    Decomposição espectral para matrizes simétricas.\n","    Usa numpy.linalg.eigh que é mais estável para A simétrica.\n","    Ideal para PCA em matriz de covariância.\n","    \"\"\"\n","    vals, vecs = np.linalg.eigh(A)\n","    idx = np.argsort(vals)[::-1]\n","    vals = vals[idx]\n","    vecs = vecs[:, idx]\n","\n","    # normaliza colunas para norma 1\n","    norms = np.linalg.norm(vecs, axis=0)\n","    norms[norms == 0] = 1.0\n","    vecs = vecs / norms\n","    return vals, vecs\n","\n","def verify_eigenpairs(A, vals, vecs, tol=1e-9):\n","    \"\"\"\n","    Verifica se A v ≈ λ v para cada par.\n","    Retorna resíduos e imprime um relatório simples.\n","    \"\"\"\n","    residuals = []\n","    for i, lam in enumerate(vals):\n","        v = vecs[:, i]\n","        left = A @ v\n","        right = lam * v\n","        r = np.linalg.norm(left - right)\n","        residuals.append(r)\n","        print(f\"λ[{i}] = {lam:.6f} | ||A v - λ v|| = {r:.3e}\")\n","    return np.array(residuals)\n","\n","# Exemplo 1: matriz simétrica 2x2\n","A = np.array([[2., 1.],\n","              [1., 2.]])\n","\n","print(\"=== Decomposição com eigh (simétrica) ===\")\n","vals_s, vecs_s = eigen_decomp_symmetric(A)\n","print(\"Autovalores:\", vals_s)\n","print(\"Autovetores (colunas):\\n\", vecs_s)\n","_ = verify_eigenpairs(A, vals_s, vecs_s)\n","\n","print(\"\\n=== Decomposição geral com eig ===\")\n","vals_g, vecs_g = eigen_decomp(A)\n","print(\"Autovalores:\", vals_g)\n","print(\"Autovetores (colunas):\\n\", vecs_g)\n","_ = verify_eigenpairs(A, vals_g, vecs_g)\n","\n","# Exemplo 2 opcional: matriz 3x3 qualquer\n","# B = np.array([[3., 2., 0.],\n","#               [2., 3., 0.],\n","#               [0., 0., 1.]])\n","# print(\"\\n=== Matriz 3x3 ===\")\n","# valsB, vecsB = eigen_decomp_symmetric(B)\n","# print(\"Autovalores:\", valsB)\n","# print(\"Autovetores (colunas):\\n\", vecsB)\n","# _ = verify_eigenpairs(B, valsB, vecsB)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_RvmpWn-fRdk","executionInfo":{"status":"ok","timestamp":1757622911857,"user_tz":180,"elapsed":22,"user":{"displayName":"Mauricio Acconcia Dias","userId":"18356211420490420198"}},"outputId":"60f06674-150e-45fd-e75e-0c6658378f2a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["=== Decomposição com eigh (simétrica) ===\n","Autovalores: [3. 1.]\n","Autovetores (colunas):\n"," [[ 0.70710678 -0.70710678]\n"," [ 0.70710678  0.70710678]]\n","λ[0] = 3.000000 | ||A v - λ v|| = 0.000e+00\n","λ[1] = 1.000000 | ||A v - λ v|| = 0.000e+00\n","\n","=== Decomposição geral com eig ===\n","Autovalores: [3. 1.]\n","Autovetores (colunas):\n"," [[ 0.70710678 -0.70710678]\n"," [ 0.70710678  0.70710678]]\n","λ[0] = 3.000000 | ||A v - λ v|| = 0.000e+00\n","λ[1] = 1.000000 | ||A v - λ v|| = 0.000e+00\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"ycL_AuqAfMOO"}},{"cell_type":"markdown","source":["## 3. Estatística básica necessária\n","\n","Esta seção revisa os conceitos estatísticos essenciais para entender PCA.\n","\n","### 3.1 Média e centralização\n","Dado um conjunto de n observações de uma variável X:\n","$$\n","\\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n} x_i\n","$$\n","A **centralização** consiste em subtrair a média de cada observação: $x_i^{c}=x_i-\\bar{x}$.\n","Para dados multivariados $X\\in\\mathbb{R}^{n\\times d}$ com colunas como variáveis, centraliza-se cada coluna.\n","\n","### 3.2 Variância\n","A variância mede a dispersão de uma variável em torno da média.\n","$$\n","\\mathrm{Var}(X)=\\frac{1}{n-1}\\sum_{i=1}^{n}\\big(x_i-\\bar{x}\\big)^2\n","$$\n","Utiliza-se $n-1$ para obter um estimador não tendencioso da variância amostral.\n","\n","### 3.3 Covariância\n","A covariância mede a variação conjunta de duas variáveis X e Y.\n","$$\n","\\mathrm{Cov}(X,Y)=\\frac{1}{n-1}\\sum_{i=1}^{n}\\big(x_i-\\bar{x}\\big)\\big(y_i-\\bar{y}\\big)\n","$$\n","Interpretação:\n","- $\\mathrm{Cov}>0$: tendem a aumentar juntas.\n","- $\\mathrm{Cov}<0$: quando uma aumenta, a outra tende a diminuir.\n","- $\\mathrm{Cov}\\approx 0$: variações independentes em média.\n","\n","### 3.4 Matriz de covariância\n","Para $X\\in\\mathbb{R}^{n\\times d}$ centralizado por colunas, a matriz de covariância $ \\Sigma \\in \\mathbb{R}^{d\\times d} $ é:\n","$$\n","\\Sigma = \\frac{1}{n-1} X^{\\top}X\n","$$\n","Elementos:\n","$$\n","\\Sigma_{jj}=\\mathrm{Var}(X_j), \\quad \\Sigma_{jk}=\\mathrm{Cov}(X_j,X_k)\n","$$\n","Propriedades:\n","- $\\Sigma$ é **simétrica** e **semidefinida positiva**.\n","- Autovalores de $\\Sigma$ são **reais** e não negativos.\n","\n","### 3.5 Correlação\n","A correlação de Pearson é a covariância normalizada pelas dispersões.\n","$$\n","\\rho_{XY}=\\frac{\\mathrm{Cov}(X,Y)}{\\sqrt{\\mathrm{Var}(X)\\,\\mathrm{Var}(Y)}}\n","$$\n","$\\rho\\in[-1,1]$. É útil para comparar associações em escalas diferentes.\n","\n","### 3.6 Padronização e escala\n","PCA é sensível à escala. Em muitos casos, padroniza-se cada variável para média zero e desvio padrão 1:\n","$$\n","z_i=\\frac{x_i-\\bar{x}}{s_x},\\quad s_x=\\sqrt{\\mathrm{Var}(X)}\n","$$\n","Em dados com unidades heterogêneas, fazer PCA na **matriz de correlação** equivale a aplicar PCA nos dados padronizados.\n","\n","### 3.7 Exemplo numérico curto\n","Considere duas variáveis $X$ e $Y$ com três observações cada:\n","$$\n","X=\\{2,4,6\\},\\qquad Y=\\{1,3,5\\}\n","$$\n","Médias:\n","$$\n","\\bar{x}=\\frac{2+4+6}{3}=4,\\quad \\bar{y}=\\frac{1+3+5}{3}=3\n","$$\n","Dados centralizados:\n","$$\n","X^c=\\{-2,0,2\\},\\qquad Y^c=\\{-2,0,2\\}\n","$$\n","Variâncias:\n","$$\n","\\mathrm{Var}(X)=\\frac{(-2)^2+0^2+2^2}{3-1}=\\frac{8}{2}=4, \\quad\n","\\mathrm{Var}(Y)=4\n","$$\n","Covariância:\n","$$\n","\\mathrm{Cov}(X,Y)=\\frac{(-2)(-2)+0\\cdot 0+2\\cdot 2}{2}=\\frac{8}{2}=4\n","$$\n","Matriz de covariância:\n","$$\n","\\Sigma=\n","\\begin{bmatrix}\n","4 & 4\\\\[4pt]\n","4 & 4\n","\\end{bmatrix}\n","$$\n","Observa-se correlação perfeita positiva $(\\rho=1)$. Todas as variações estão ao longo de uma única direção, o que antecipa a ideia do PCA: um componente principal explica 100% da variância neste exemplo idealizado.\n","\n","### 3.8 Observações práticas\n","- Outliers podem inflar variâncias e covariâncias. Em cenários robustos, considera-se pré-processamento ou alternativas robustas.\n","- Dados faltantes exigem imputação ou métodos que lidem com incompletude antes do PCA.\n","- Em pipelines de aprendizado de máquina, recomenda-se aplicar standard scaler antes do PCA quando existirem escalas distintas entre variáveis.\n"],"metadata":{"id":"oj4EIJkhfNgV"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"2gpKc6Wuf5EL"}},{"cell_type":"markdown","source":["## 4. Do conceito à intuição do PCA\n","\n","O PCA busca encontrar as direções no espaço dos dados onde a variabilidade é máxima.  \n","Essas direções são determinadas pelos **autovetores da matriz de covariância**, e a quantidade de variância explicada em cada direção é dada pelos **autovalores** correspondentes.\n","\n","---\n","\n","### 4.1 Direção de maior variância\n","- Imagine um conjunto de pontos no plano (2D) distribuídos em forma de nuvem.\n","- A direção ao longo da qual esses pontos se espalham mais é o **primeiro componente principal**.\n","- Essa direção corresponde ao autovetor associado ao maior autovalor da matriz de covariância.\n","\n","---\n","\n","### 4.2 Rotação do sistema de coordenadas\n","- O PCA equivale a **rotacionar o sistema de eixos** para alinhar os dados às direções de máxima variância.\n","- No novo sistema:\n","  - O primeiro eixo (PC1) explica a maior parte da variabilidade.\n","  - O segundo eixo (PC2) explica a maior parte do que sobra, e assim por diante.\n","\n","Geometricamente:\n","- Antes: eixos fixos nos eixos originais (variáveis brutas).\n","- Depois: eixos girados de acordo com as direções de maior espalhamento dos dados.\n","\n","---\n","\n","### 4.3 Relação com autovalores e autovetores\n","- Seja $\\Sigma$ a matriz de covariância dos dados centralizados.\n","- Autovalores $\\lambda_i$ e autovetores $\\vec{v}_i$ de $\\Sigma$ satisfazem:\n","$$\n","\\Sigma \\vec{v}_i = \\lambda_i \\vec{v}_i\n","$$\n","- Interpretação:\n","  - $\\vec{v}_i$: direção do i-ésimo componente principal.\n","  - $\\lambda_i$: variância explicada ao longo dessa direção.\n","\n","---\n","\n","### 4.4 Exemplo em 2D (conceitual)\n","- Suponha dados em duas variáveis $X$ e $Y$ com forte correlação positiva.\n","- A matriz de covariância terá um autovalor grande e outro pequeno.\n","- O autovetor do maior autovalor aponta para a diagonal $y=x$.\n","- Isso significa que a maior variação está ao longo dessa linha.\n","- O segundo autovetor (diagonal $y=-x$) explica pouca variância.\n","\n","---\n","\n","### 4.5 Variação explicada\n","- A proporção de variância explicada por cada componente é:\n","$$\n","\\text{Explained Variance Ratio}_i = \\frac{\\lambda_i}{\\sum_j \\lambda_j}\n","$$\n","- Essa métrica é usada para decidir quantos componentes manter.\n","- Em muitos casos, poucos componentes retêm a maior parte da variância total.\n","\n","---\n","\n","### 4.6 Intuição prática\n","- PCA não “cria” informação nova, apenas reorganiza o espaço para destacar o que mais varia.\n","- Se duas variáveis são redundantes (altamente correlacionadas), PCA pode condensá-las em um único eixo.\n","- Isso simplifica a análise sem perda significativa de informação.\n"],"metadata":{"id":"dDL9Yiynf6E4"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"dkXD-xO9gXbE"}},{"cell_type":"markdown","source":["## 5. Passo a passo do PCA (teórico)\n","\n","Objetivo: encontrar uma base ortogonal em que a variância dos dados seja maximizada nos primeiros eixos, permitindo reduzir dimensões com mínima perda de informação.\n","\n","### 5.1 Preparação dos dados\n","1) Organize os dados em uma matriz $X \\in \\mathbb{R}^{n \\times d}$, linhas como amostras e colunas como variáveis.  \n","2) Centralize cada coluna: subtraia a média de cada variável. Obtenha $X_c$.\n","\n","### 5.2 Matriz de covariância\n","Calcule a matriz de covariância $\\Sigma \\in \\mathbb{R}^{d \\times d}$:\n","$$\n","\\Sigma = \\frac{1}{n-1} X_c^{\\top} X_c\n","$$\n","$\\Sigma$ é simétrica e semidefinida positiva.\n","\n","### 5.3 Decomposição espectral\n","Obtenha autovalores $\\lambda_i$ e autovetores $\\vec{v}_i$ de $\\Sigma$:\n","$$\n","\\Sigma \\vec{v}_i = \\lambda_i \\vec{v}_i\n","$$\n","Ordene autovalores de forma decrescente $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_d$. Os $\\vec{v}_i$ formam uma base ortonormal.\n","\n","### 5.4 Componentes principais e variância explicada\n","O i-ésimo componente principal é a direção $\\vec{v}_i$.  \n","A variância explicada por ele é $\\lambda_i$. A fração explicada é:\n","$$\n","\\text{EVR}_i = \\frac{\\lambda_i}{\\sum_{j=1}^{d} \\lambda_j}\n","$$\n","\n","### 5.5 Projeção dos dados\n","Projete os dados centralizados nos componentes principais:\n","$$\n","Z = X_c V\n","$$\n","onde $V=[\\vec{v}_1\\ \\vec{v}_2\\ \\dots\\ \\vec{v}_d]$. As colunas de $Z$ são as coordenadas nas novas direções.\n","\n","### 5.6 Redução de dimensionalidade\n","Escolha $k < d$. Use $V_k=[\\vec{v}_1\\ \\dots\\ \\vec{v}_k]$ e $Z_k=X_c V_k$.  \n","Reconstrução aproximada em $d$ dimensões:\n","$$\n","\\widehat{X} = Z_k V_k^{\\top} + \\text{média}\n","$$\n","\n","---\n","\n","## 5.7 Exemplo numérico simples\n","\n","Dados em 2 variáveis, 5 amostras:\n","$$\n","X=\n","\\begin{bmatrix}\n","2 & 0\\\\\n","0 & 2\\\\\n","3 & 1\\\\\n","4 & 3\\\\\n","5 & 5\n","\\end{bmatrix}\n","$$\n","\n","Média por coluna:\n","$$\n","\\bar{x}=[2{,}8,\\ 2{,}2]\n","$$\n","\n","Centralização $X_c = X - \\bar{x}$:\n","$$\n","X_c=\n","\\begin{bmatrix}\n","-0{,}8 & -2{,}2\\\\\n","-2{,}8 & -0{,}2\\\\\n","\\ \\ 0{,}2 & -1{,}2\\\\\n","\\ \\ 1{,}2 & \\ \\ 0{,}8\\\\\n","\\ \\ 2{,}2 & \\ \\ 2{,}8\n","\\end{bmatrix}\n","$$\n","\n","Matriz de covariância $\\Sigma=\\frac{1}{n-1}X_c^\\top X_c$ com $n=5$:\n","$$\n","\\Sigma=\n","\\begin{bmatrix}\n","3{,}7 & 2{,}3\\\\[2pt]\n","2{,}3 & 3{,}7\n","\\end{bmatrix}\n","$$\n","\n","Autovalores e autovetores de $\\Sigma$:\n","$$\n","\\lambda_1=6{,}0,\\quad \\lambda_2=1{,}4\n","$$\n","$$\n","\\vec{v}_1=\\frac{1}{\\sqrt{2}}\\begin{bmatrix}1\\\\[2pt]1\\end{bmatrix},\\quad\n","\\vec{v}_2=\\frac{1}{\\sqrt{2}}\\begin{bmatrix}-1\\\\[2pt]1\\end{bmatrix}\n","$$\n","\n","Variância explicada:\n","$$\n","\\text{EVR}_1=\\frac{6}{7{,}4}\\approx 0{,}8108\\quad (81{,}08\\%),\\qquad\n","\\text{EVR}_2=\\frac{1{,}4}{7{,}4}\\approx 0{,}1892\\quad (18{,}92\\%)\n","$$\n","\n","Projeção $Z=X_c V$ resulta aproximadamente em:\n","$$\n","Z \\approx\n","\\begin{bmatrix}\n","-2{,}1213 & -0{,}9899\\\\\n","-2{,}1213 & \\ \\ 1{,}8385\\\\\n","-0{,}7071 & -0{,}9899\\\\\n","\\ \\ 1{,}4142 & -0{,}2828\\\\\n","\\ \\ 3{,}5355 & \\ \\ 0{,}4243\n","\\end{bmatrix}\n","$$\n","\n","Se escolhermos $k=1$, mantemos apenas $\\vec{v}_1$. A reconstrução $\\widehat{X}=Z_1 \\vec{v}_1^\\top + \\bar{x}$ aproxima os dados originais usando uma única dimensão, retendo cerca de 81 por cento da variância total.\n","\n","---\n","\n","## 5.8 Implementação em Python\n","\n","### 5.8.1 PCA do zero com NumPy\n","\n"],"metadata":{"id":"I9LT3CLQgYgD"}},{"cell_type":"code","source":["import numpy as np\n","\n","def pca_numpy(X, k=None, scale=False):\n","    \"\"\"\n","    PCA do zero usando NumPy.\n","    X: matriz n x d com amostras em linhas.\n","    k: número de componentes a manter. Se None, mantém todos.\n","    scale: se True, padroniza colunas para média 0 e desvio 1 antes do PCA.\n","    Retorna: mean, components, eigenvalues, explained_variance_ratio, scores, X_recon (se k não None).\n","    \"\"\"\n","    X = np.asarray(X, dtype=float)\n","    n, d = X.shape\n","\n","    # opcional: padronização\n","    if scale:\n","        mu = X.mean(axis=0)\n","        sigma = X.std(axis=0, ddof=1)\n","        sigma[sigma == 0] = 1.0\n","        Xc = (X - mu) / sigma\n","        postprocess = lambda Y: Y * sigma + mu\n","    else:\n","        mu = X.mean(axis=0)\n","        Xc = X - mu\n","        postprocess = lambda Y: Y + mu\n","\n","    # matriz de covariância d x d\n","    Sigma = (Xc.T @ Xc) / (n - 1)\n","\n","    # autovalores e autovetores para matriz simétrica\n","    vals, vecs = np.linalg.eigh(Sigma)\n","\n","    # ordenar decrescente\n","    idx = np.argsort(vals)[::-1]\n","    vals = vals[idx]\n","    vecs = vecs[:, idx]\n","\n","    # componentes principais: colunas de vecs\n","    V = vecs\n","\n","    # scores: coordenadas projetadas\n","    Z = Xc @ V\n","\n","    # razão de variância explicada\n","    evr = vals / vals.sum()\n","\n","    # se k definido, reduzir e reconstruir\n","    X_recon = None\n","    if k is not None:\n","        Vk = V[:, :k]\n","        Zk = Xc @ Vk\n","        X_recon = postprocess(Zk @ Vk.T)\n","\n","    return mu, V, vals, evr, Z, X_recon\n","\n","# Exemplo com os dados do item 5.7\n","X = np.array([\n","    [2., 0.],\n","    [0., 2.],\n","    [3., 1.],\n","    [4., 3.],\n","    [5., 5.]\n","])\n","\n","mu, V, vals, evr, Z, Xr = pca_numpy(X, k=1, scale=False)\n","print(\"Média:\", mu)\n","print(\"Autovalores:\", vals)\n","print(\"Variância explicada:\", evr)\n","print(\"Componentes (colunas):\\n\", V)\n","print(\"Scores Z (primeiras linhas):\\n\", Z[:5])\n","print(\"Reconstrução k=1 (primeiras linhas):\\n\", Xr[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_zDpFdHDg2s0","executionInfo":{"status":"ok","timestamp":1757623326218,"user_tz":180,"elapsed":28,"user":{"displayName":"Mauricio Acconcia Dias","userId":"18356211420490420198"}},"outputId":"7bef3156-3253-49bf-d353-95bea9f357fa"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Média: [2.8 2.2]\n","Autovalores: [6.  1.4]\n","Variância explicada: [0.81081081 0.18918919]\n","Componentes (colunas):\n"," [[ 0.70710678 -0.70710678]\n"," [ 0.70710678  0.70710678]]\n","Scores Z (primeiras linhas):\n"," [[-2.12132034 -0.98994949]\n"," [-2.12132034  1.83847763]\n"," [-0.70710678 -0.98994949]\n"," [ 1.41421356 -0.28284271]\n"," [ 3.53553391  0.42426407]]\n","Reconstrução k=1 (primeiras linhas):\n"," [[1.3 0.7]\n"," [1.3 0.7]\n"," [2.3 1.7]\n"," [3.8 3.2]\n"," [5.3 4.7]]\n"]}]},{"cell_type":"markdown","source":["## 6. Análise do algoritmo PCA\n","\n","### 6.1 Pipeline e custos computacionais\n","Considere \\(X \\in \\mathbb{R}^{n \\times d}\\) com amostras em linhas e variáveis em colunas.\n","\n","1) Centralização das colunas  \n","Custo de tempo: \\(O(nd)\\). Custo de memória: \\(O(d)\\) para guardar as médias.\n","\n","2) Opcional: padronização por desvio padrão  \n","Custo de tempo: \\(O(nd)\\). Importante quando as variáveis estão em escalas diferentes.\n","\n","3) Cálculo de \\(\\Sigma = \\frac{1}{n-1}X_c^\\top X_c\\)  \n","Custo de tempo: \\(O(nd^2)\\). Custo de memória: \\(O(d^2)\\).\n","\n","4) Autodecomposição de \\(\\Sigma\\) (simétrica)  \n","Custo de tempo: \\(O(d^3)\\) usando métodos clássicos. Retorna autovalores \\(\\lambda_i\\) e autovetores \\(v_i\\).\n","\n","5) Ordenação dos autovalores e autovetores  \n","Custo de tempo: \\(O(d \\log d)\\).\n","\n","6) Projeção nos \\(k\\) primeiros componentes  \n","\\(Z = X_c V_k\\) com custo \\(O(ndk)\\).\n","\n","7) Reconstrução aproximada  \n","\\(\\widehat{X} = Z V_k^\\top + \\text{média}\\) com custo \\(O(ndk)\\).\n","\n","Resumo prático: para \\(d\\) moderado, o gargalo é a autodecomposição \\(O(d^3)\\). Para \\(d\\) grande, formar \\(\\Sigma\\) já custa \\(O(nd^2)\\).\n","\n","### 6.2 PCA via SVD em vez de autodecomposição\n","A SVD de \\(X_c\\) evita formar \\(\\Sigma\\) e é mais estável numericamente.\n","\n","\\[\n","X_c = U S V^\\top \\quad \\Rightarrow \\quad \\Sigma = \\frac{1}{n-1} V S^2 V^\\top\n","\\]\n","\n","Componentes principais são as colunas de \\(V\\). Autovalores são \\(S^2/(n-1)\\).  \n","Vantagem: estabilidade numérica superior e possibilidade de SVD truncada para \\(k\\) componentes.\n","\n","Complexidade típica da SVD: \\(O(nd^2)\\) quando \\(n \\ge d\\) e \\(O(n^2 d)\\) quando \\(n \\le d\\).  \n","Para bases grandes, usar SVD econômica ou aleatorizada.\n","\n","### 6.3 Caso \\(n \\ll d\\) e formulação dual\n","Quando há muito mais variáveis que amostras, é eficiente diagonalizar \\(X_c X_c^\\top \\in \\mathbb{R}^{n \\times n}\\).  \n","Obtém-se \\(U\\) e \\(S\\), e depois \\(V = X_c^\\top U S^{-1}\\) com normalização adequada.\n","\n","### 6.4 Escala das variáveis\n","PCA é sensível à escala. Quando variáveis possuem unidades diferentes, padronizar para média zero e desvio um é recomendável.  \n","Alternativamente, aplicar PCA sobre a matriz de correlação equivale a padronizar antes.\n","\n","### 6.5 Seleção de \\(k\\)\n","Critérios usuais:\n","- Variância explicada acumulada maior que um limiar, por exemplo 90 por cento ou 95 por cento.  \n","- Gráfico de cotovelo da variância explicada.  \n","- Validação de tarefa final, por exemplo desempenho de um modelo após redução.\n","\n","### 6.6 Erro de reconstrução\n","Para dados centralizados, a soma dos autovalores equivale à variância total.  \n","Ao reter \\(k\\) componentes, o erro quadrático médio de reconstrução por amostra é a soma dos autovalores descartados.  \n","Isto fornece uma medida direta da perda ao reduzir a dimensionalidade.\n","\n","### 6.7 Robustez e dados reais\n","- Outliers podem distorcer variâncias e direções. Pré-processamento e versões robustas de PCA podem ser necessários.  \n","- Dados faltantes exigem imputação antes do PCA tradicional.  \n","- A direção dos autovetores possui ambiguidade de sinal. Inverter o sinal de um componente não altera o subespaço.\n","\n","### 6.8 Interpretação dos componentes\n","- Componentes são combinações lineares das variáveis originais.  \n","- Cargas fatoriais podem ser interpretadas pelas correlações das variáveis com cada componente.  \n","- Para interpretabilidade, observar magnitude e sinal das entradas dos autovetores ou usar gráficos biplot.\n","\n","### 6.9 Memória\n","- Armazenar \\(X\\) em float64 consome aproximadamente \\(8nd\\) bytes.  \n","- Armazenar \\(\\Sigma\\) consome \\(8d^2\\) bytes.  \n","- Em \\(d\\) muito grande, preferir SVD direta com esquemas econômicos para reduzir uso de memória.\n"],"metadata":{"id":"bBoVBJV8h7Rm"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"7J6yNm1yiB9r"}},{"cell_type":"markdown","source":["## 7. Análise dos resultados no exemplo da Seção 5.7\n","\n","### 7.1 Recapitulação dos números\n","Para os dados 2D fornecidos, a matriz de covariância foi\n","$$\n","\\Sigma=\n","\\begin{bmatrix}\n","3{,}7 & 2{,}3\\\\\n","2{,}3 & 3{,}7\n","\\end{bmatrix}\n","$$\n","Autovalores e autovetores\n","$$\n","\\lambda_1=6{,}0,\\quad \\lambda_2=1{,}4\n","$$\n","$$\n","v_1=\\frac{1}{\\sqrt{2}}\\begin{bmatrix}1\\\\ 1\\end{bmatrix},\\quad\n","v_2=\\frac{1}{\\sqrt{2}}\\begin{bmatrix}-1\\\\ 1\\end{bmatrix}\n","$$\n","\n","### 7.2 Interpretação geométrica\n","- O primeiro componente principal aponta para a diagonal $y=x$.  \n","- Isto indica que a maior parte da variação ocorre ao longo da soma das variáveis.  \n","- O segundo componente, na direção $y=-x$, mede discrepâncias entre as variáveis.\n","\n","### 7.3 Variância explicada e escolha de $k$\n","Variância total é $\\lambda_1+\\lambda_2=7{,}4$.  \n","Proporções\n","$$\n","\\text{EVR}_1 \\approx 81{,}08\\% \\quad \\text{e} \\quad \\text{EVR}_2 \\approx 18{,}92\\%.\n","$$\n","- Se o critério é 80 por cento, $k=1$ é suficiente.  \n","- Se o critério é 90 por cento, é necessário $k=2$.\n","\n","### 7.4 Projeções e sinais\n","As coordenadas projetadas $Z$ que aparecem no código podem diferir por um sinal global em cada componente.  \n","Isto é esperado, pois autovetores têm ambiguidade de sinal.  \n","A interpretação não muda.\n","\n","### 7.5 Reconstrução e erro\n","Com $k=1$ a reconstrução aproxima $X$ a partir de uma única coordenada ao longo de $v_1$.  \n","Erro quadrático médio de reconstrução por amostra é igual à soma dos autovalores descartados  \n","$$\n","\\text{MSE} = \\lambda_2 = 1{,}4\n","$$\n","que corresponde a aproximadamente 18,9 por cento da variância total.  \n","Conseqüência prática: boa compressão para visualizar ou simplificar, com perda moderada.\n","\n","### 7.6 Cargas e interpretação\n","Como $v_1$ possui entradas positivas e de mesma magnitude, o primeiro componente corresponde a uma tendência comum das duas variáveis.  \n","Já $v_2$ captura diferenças entre as variáveis.  \n","Esta leitura é coerente com os dados, que crescem de forma aproximadamente conjunta, sobretudo nos últimos pontos.\n","\n","### 7.7 Decisões para uso prático\n","- Para visualização, $k=1$ já resume a estrutura dominante.  \n","- Para reconstrução com baixa perda, manter $k=2$.  \n","- Em conjuntos com escalas heterogêneas, repetir a análise após padronização para verificar estabilidade das conclusões.\n","\n","### 7.8 Próximos passos sugeridos\n","- Plotar gráfico de cotovelo com a variância explicada acumulada.  \n","- Construir biplot com variáveis e amostras para reforçar a leitura das cargas.  \n","- Testar robustez com remoção de outliers e com padronização.\n"],"metadata":{"id":"hCWDO190iCnF"}},{"cell_type":"markdown","source":["## 8. Conclusão\n","\n","A Análise de Componentes Principais (PCA) é uma ferramenta poderosa para compreender e simplificar conjuntos de dados de alta dimensão.  \n","Ao longo deste notebook, percorremos um caminho completo:\n","\n","- Revisamos fundamentos de álgebra linear (autovalores, autovetores) e estatística (variância, covariância).\n","- Construímos passo a passo o algoritmo do PCA: centralização, matriz de covariância, decomposição espectral e projeção.\n","- Implementamos o PCA manualmente em Python e também com `scikit-learn`, comparando os resultados.\n","- Interpretamos a variância explicada e o papel de cada componente principal.\n","- Discutimos critérios para escolha de $k$, impacto na reconstrução e cuidados com escalas e outliers.\n","\n","O PCA, portanto, não apenas reduz dimensionalidade mas também ajuda a **interpretar padrões latentes** nos dados, identificar redundâncias e melhorar a eficiência de modelos de aprendizado de máquina.\n","\n","### Limitações\n","- Assume **linearidade**: não captura relações não lineares entre variáveis.\n","- É sensível à **escala das variáveis** e a outliers.\n","- Direções encontradas são baseadas exclusivamente em variância, não em relevância para uma tarefa de predição.\n","\n","### Próximos passos\n","- Explorar **Kernel PCA** para relações não lineares.\n","- Testar **t-SNE** ou **UMAP** para visualização em alta dimensão.\n","- Avaliar PCA em datasets reais e verificar ganho em desempenho de modelos.\n","\n","Em resumo, o PCA é uma técnica essencial no arsenal de estatística multivariada e aprendizado de máquina, fornecendo uma base sólida para compressão de dados, pré-processamento e exploração visual.\n"],"metadata":{"id":"gSVpRWYJigVY"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"ziOAOgQRil5c"}}]}